{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С помощью модуля pandas преобразуйте файл из .xlsx в .csv формат файл ./src/online_retail.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортирование необходимой библиотеки\n",
    "import pandas as pd \n",
    "# Чтение файла XLSX с помощью функции read_excel().\n",
    "# Указать путь к файлу, имя листа (если их несколько) и другие параметры.\n",
    "df = pd.read_excel('./src/online_retail.xlsx', sheet_name='Online Retail', header=0)\n",
    "# Запись созданного объекта DataFrame в файл CSV с помощью функции to_csv()\n",
    "df.to_csv('./src/online_retail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "from pyspark.sql.functions import when, array_contains, filter\n",
    "from pyspark.sql.functions import col, lit, asc, desc, round, current_date, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация Spark-сессии\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"onlineretail\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:00:53 WARN TaskSetManager: Stage 72 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:00:57 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 72 (TID 43): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Создание dataframe из скачанного файла\n",
    "sparkDF_orig = spark.createDataFrame(df) \n",
    "sparkDF_orig.printSchema()\n",
    "sparkDF_orig.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитайте следующие показатели:\n",
    "\n",
    "    -Количество строк в файле\n",
    "\n",
    "    -Количество уникальных клиентов\n",
    "\n",
    "    -В какой стране совершается большинство покупок\n",
    "    \n",
    "    -Даты самой ранней и самой последней покупки на платформе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o266.count.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3613)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Чтобы посчитать количество строк в dataframe на PySpark, можно использовать функцию count():\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m line_count \u001b[38;5;241m=\u001b[39m \u001b[43msparkDF_orig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКоличество строк:\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_count)\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o266.count.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3613)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# Чтобы посчитать количество строк в dataframe на PySpark, можно использовать функцию count():\n",
    "line_count = sparkDF_orig.count()\n",
    "print(\"Количество строк:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sparkDF_NotNull \u001b[38;5;241m=\u001b[39m sparkDF_orig\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Удаление из датафрейма строк, которые содержат некорректные значения стоимости и количества товара\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sparkDF \u001b[38;5;241m=\u001b[39m \u001b[43msparkDF_NotNull\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkDF_NotNull\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkDF_NotNull\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnitPrice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCustomerID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКоличество строк после удаления ненужных: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sparkDF\u001b[38;5;241m.\u001b[39mcount())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sparkDF.show(3)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/dataframe.py:2740\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort\u001b[39m(\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: Union[\u001b[38;5;28mstr\u001b[39m, Column, List[Union[\u001b[38;5;28mstr\u001b[39m, Column]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001b[39;00m\n\u001b[1;32m   2653\u001b[0m \n\u001b[1;32m   2654\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2740\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/dataframe.py:2777\u001b[0m, in \u001b[0;36mDataFrame._sort_cols\u001b[0;34m(self, cols, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2776\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2777\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnOrName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m   2778\u001b[0m ascending \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascending\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/column.py:55\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39mfunctions\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[0;32m~/.conda/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/utils.py:248\u001b[0m, in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext or SparkSession should be created first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "# Предвариьная обработка датафрейма\n",
    "# Удаление из датафрейма строк, которые не содержат значения в столбце CustomerID\n",
    "sparkDF_NotNull = sparkDF_orig.dropna(subset=\"CustomerID\")\n",
    "\n",
    "# Удаление из датафрейма строк, которые содержат некорректные значения стоимости и количества товара\n",
    "sparkDF = sparkDF_NotNull.filter((sparkDF_NotNull.Quantity > 0) & (sparkDF_NotNull.UnitPrice > 0)).orderBy(\"CustomerID\")\n",
    "print(\"Количество строк после удаления ненужных: \", sparkDF.count())\n",
    "# sparkDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:08 WARN TaskSetManager: Stage 80 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT CustomerID)|\n",
      "+--------------------------+\n",
      "|                      4338|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Количество уникальных клиентов\n",
    "countUniqCustomerID = sparkDF.select(countDistinct(\"CustomerID\"))\n",
    "countUniqCustomerID.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:12 WARN TaskSetManager: Stage 86 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 86:==========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|       Country|countCountry|\n",
      "+--------------+------------+\n",
      "|United Kingdom|      354321|\n",
      "+--------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# В какой стране совершается большинство покупок\n",
    "# Группировка по стране (колонке \"Country\") и подсчёт количества строк по каждой стране\n",
    "dfGroupByCountry = sparkDF.groupBy('Country').count()\\\n",
    "    .select(col(\"Country\"), col(\"count\").alias(\"countCountry\"))\n",
    "# dfGroupByCountry.show(3)\n",
    "# сортировка полученного датафрэйма по убыванию по количеству покупок в каждой стране.\n",
    "orderedDF = dfGroupByCountry.orderBy(col(\"count\").desc())\n",
    "# вывод только первой строки, в которой указана страна, в которой совершается большинство покупок\n",
    "orderedDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:15 WARN TaskSetManager: Stage 89 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   min(InvoiceDate)|\n",
      "+-------------------+\n",
      "|2010-12-01 08:26:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:18 WARN TaskSetManager: Stage 92 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 92:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   max(InvoiceDate)|\n",
      "+-------------------+\n",
      "|2011-12-09 12:50:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Даты самой ранней и самой последней покупки на платформе\n",
    "# строка с самой ранней покупкой на платформе\n",
    "dateDFmin = sparkDF.orderBy(col(\"InvoiceDate\").asc()).limit(1)\n",
    "#dateDFmin.show(1)\n",
    "# дата самой ранней покупки на платформе\n",
    "dateMin = sparkDF.select(min(sparkDF.InvoiceDate))\n",
    "dateMin.show()\n",
    "\n",
    "# строка с самой последней покупкой на платформе\n",
    "dateDFmax = sparkDF.orderBy(col(\"InvoiceDate\").desc()).limit(1)\n",
    "#dateDFmax.show()\n",
    "# дата самой последней покупки на платформе\n",
    "dateMax = sparkDF.select(max(sparkDF.InvoiceDate))\n",
    "dateMax.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите RFM-анализ клиентов платформы. \n",
    "Что такое RFM-анализ? \n",
    "  Обычно RFM-анализ используется в маркетинге для оценки ценности клиента на основе его:\n",
    "  * Recency - Давность: как давно каждый покупатель совершил покупку?\n",
    "    \n",
    "  * Frequency- Частота: Как часто они что-то покупали?\n",
    "    \n",
    "  * Monetary - Денежная ценность: сколько денег они в среднем тратят при совершении покупок?\n",
    "\n",
    "Добавьте в dataframe для каждого клиента 3 новых поля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:21 WARN TaskSetManager: Stage 95 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 95:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|         TotalCost|CurrentDate|DateDifference|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----------+--------------+\n",
      "|   541431|    23166|MEDIUM CERAMIC TO...|   74215|2011-01-18 10:01:00|     1.04|   12346.0|United Kingdom|           77183.6| 2024-07-07|          4919|\n",
      "|   537626|    85116|BLACK CANDELABRA ...|      12|2010-12-07 14:57:00|      2.1|   12347.0|       Iceland|25.200000000000003| 2024-07-07|          4961|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Добавление колонки с общей стоимостью товара (цена*количество) \n",
    "# На основе этой колонки будет рассчитываться показатель Monetary\n",
    "dfWithTotalCost = sparkDF.withColumn(\"TotalCost\", sparkDF.UnitPrice * sparkDF.Quantity)\\\n",
    "    .withColumn(\"CurrentDate\", current_date())\n",
    "# Добаваление колонки, содержащей количество дней между текущей датой и датой самой поздней покупки\n",
    "dfWithTotalCostAndDateDif = dfWithTotalCost.withColumn(\"DateDifference\", datediff(dfWithTotalCost.CurrentDate, dfWithTotalCost.InvoiceDate))\n",
    "dfWithTotalCostAndDateDif.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для распределения клиентов по группам (ABC) выбираем следующий критерий:\n",
    "\n",
    "1) Если значение показателя для клиента лучше среднего значения данного показателя и входит в интервал от 10 процентов разницы (по абсолютному значению, т.е. по модулю) лучшего и среднего значений до лучшего значения, \n",
    "ТО определеяем клиента в группу \"A\".\n",
    "\n",
    "2) Если значение показателя для клиента лучше среднего значения и входит в интервал от среднего значения до 10 процентов разницы (по модулю) лучшего и среднего значений, \n",
    "ЛИБО\n",
    "если значение показателя для клиента хуже среднего значения и входит в интервал от 60 процентов разницы (по модулю) худшего и среднего значения до среднего значения,\n",
    "ТО определеяем клиента в группу \"B\".\n",
    "\n",
    "3) Если значение показателя для клиента хуже среднего значения и входит в интервал от худшего значения до 60 процентов разницы (по модулю) худшего и среднего значений,\n",
    "ТО определеяем клиента в группу \"C\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:24 WARN TaskSetManager: Stage 96 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:27 WARN TaskSetManager: Stage 108 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:30 WARN TaskSetManager: Stage 120 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:33 WARN TaskSetManager: Stage 121 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Расчёт показателя Recency (Давность) на основе поиска максимального значения даты совершения покупки клиентом\n",
    "dfRecency = dfWithTotalCostAndDateDif.groupBy('CustomerID')\\\n",
    "    .agg(max('DateDifference').alias('Recency')).orderBy(\"CustomerID\")\n",
    "\n",
    "# Определение максимального, минимального и среднего значения Recency (позволит приближенно оценить распределение признака)\n",
    "dfRecencyMaxMin = dfRecency.select(max(\"Recency\").alias(\"maxRec\"),\\\n",
    "                                   min(\"Recency\").alias(\"minRec\"),\\\n",
    "                                   round(avg(\"Recency\")).alias(\"avgRec\"))   # функция round - для округления\n",
    "maxRec = dfRecencyMaxMin.collect()[0].__getitem__('maxRec')\n",
    "minRec = dfRecencyMaxMin.collect()[0].__getitem__('minRec')\n",
    "avgRec = dfRecencyMaxMin.collect()[0].__getitem__('avgRec')\n",
    "# Добавление колонки для указания группы покупателей по показателю Recency (Давность)\n",
    "dfRecencyGroupABC = dfRecency\\\n",
    "    .withColumn(\"RecencyGroup\",\\\n",
    "        when((dfRecency.Recency < avgRec) \\\n",
    "                & ((avgRec - dfRecency.Recency) <= (avgRec - minRec)*0.9)\\\n",
    "            , \"A\")\\\n",
    "        .when((dfRecency.Recency < avgRec) \\\n",
    "                | ((dfRecency.Recency >= avgRec)\\\n",
    "                    & ((dfRecency.Recency - avgRec) <= (maxRec - avgRec)*0.6))\\\n",
    "            , \"B\")\\\n",
    "        .otherwise(\"C\"))\n",
    "\n",
    "# Расчёт показателя Frequency на основе количества транзакций, совершённых клиентом\n",
    "dfFrequency = dfWithTotalCostAndDateDif.groupBy(\"CustomerID\").count()\\\n",
    "    .withColumnRenamed('count', 'Frequency')\\\n",
    "    .orderBy(\"CustomerID\")\n",
    "# Определение максимального, минимального и среднего значения Frequency (позволит приближенно оценить распределение признака)\n",
    "dfFrequencyMaxMin = dfFrequency.select(max(\"Frequency\").alias(\"maxFreq\"),\\\n",
    "                                       min(\"Frequency\").alias(\"minFreq\"),\\\n",
    "                                       round(avg(\"Frequency\")).alias(\"avgFreq\"))\n",
    "maxFreq = dfFrequencyMaxMin.collect()[0].__getitem__('maxFreq')\n",
    "minFreq = dfFrequencyMaxMin.collect()[0].__getitem__('minFreq')\n",
    "avgFreq = dfFrequencyMaxMin.collect()[0].__getitem__('avgFreq')\n",
    "# Добавление колонки для указания группы покупателей по показателю Frequency (Частота покупок)\n",
    "dfFrequencyGroupABC = dfFrequency\\\n",
    "    .withColumn(\"FrequencyGroup\",\\\n",
    "        when((dfFrequency.Frequency > avgFreq)\\\n",
    "              & ((dfFrequency.Frequency - avgFreq) >= (maxFreq - avgFreq)*0.1)\\\n",
    "              , \"A\")\\\n",
    "        .when((dfFrequency.Frequency > avgFreq)\\\n",
    "                | ((dfFrequency.Frequency <= avgFreq)\\\n",
    "                    & ((avgFreq - dfFrequency.Frequency) >= (avgFreq - minFreq)*0.6))\\\n",
    "            , \"B\")\\\n",
    "        .otherwise(\"C\"))\n",
    "\n",
    "# Расчёт показателя Monetary на основе суммарных затрат на покупки по каждому клиенту\n",
    "dfMonetary = dfWithTotalCostAndDateDif.groupBy(\"CustomerID\").agg(sum(\"TotalCost\").alias('Monetary'))\\\n",
    "    .orderBy(\"CustomerID\")\n",
    "# Определение максимального, минимального и среднего значения Monetary (позволит приближенно оценить распределение признака)\n",
    "dfMonetaryMaxMin = dfMonetary.select(round(max(\"Monetary\")).alias(\"maxMon\"),\\\n",
    "                                     round(min(\"Monetary\")).alias(\"minMon\"),\\\n",
    "                                     round(avg(\"Monetary\")).alias(\"avgMon\"))\n",
    "maxMon = dfMonetaryMaxMin.collect()[0].__getitem__('maxMon')\n",
    "minMon = dfMonetaryMaxMin.collect()[0].__getitem__('minMon')\n",
    "avgMon = dfMonetaryMaxMin.collect()[0].__getitem__('avgMon')\n",
    "# Добавление колонки для указания группы покупателей по показателю Monetary (Стоимость всех покупок клиента)\n",
    "dfMonetaryGroupABC = dfMonetary\\\n",
    "    .withColumn(\"MonetaryGroup\", \\\n",
    "        when((dfMonetary.Monetary > avgMon)\\\n",
    "              & ((dfMonetary.Monetary - avgMon) >= (maxMon - avgMon)*0.1)\\\n",
    "              , \"A\")\\\n",
    "        .when((dfMonetary.Monetary > avgMon)\\\n",
    "                | ((dfMonetary.Monetary <= avgMon)\\\n",
    "                    & ((avgMon - dfMonetary.Monetary) >= (avgMon - minMon)*0.6))\\\n",
    "            , \"B\")\\\n",
    "        .otherwise(\"C\"))                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого показателя добавьте стобец с разбиением клиентов на 3 группы. \n",
    "  - Допустим, у нас есть 3 клиента, первый клиент последний раз купил товар \n",
    "    только в прошлом году, второй клиент в прошлом месяце, а третий клиент \n",
    "    на прошлой неделе. Каждый из этих клиентов должен получить различные \n",
    "    значения группы для показателя Recency - A, B и С, где А - отражает \n",
    "    наибольшую “ценность”, а С - соответственно, наименьшую. \n",
    "  - Добавьте итоговый столбец с “суммой” значений групп по каждому показателю \n",
    "    и сохраните в отдельный csv-файл Id только тех клиентов, у которых значения \n",
    "    групп ААА."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:38 WARN TaskSetManager: Stage 133 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:40 WARN TaskSetManager: Stage 134 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:42 WARN TaskSetManager: Stage 135 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:44 WARN TaskSetManager: Stage 136 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "|CustomerID|Recency|RecencyGroup|Frequency|FrequencyGroup|         Monetary|MonetaryGroup|\n",
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "|   12346.0|   4919|           C|        1|             B|          77183.6|            A|\n",
      "|   12347.0|   4961|           C|      182|             B|4309.999999999997|            B|\n",
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Объединение показателей в одну таблицу.\n",
    "dfRecencyFrequencyMonetary = dfRecencyGroupABC\\\n",
    "    .join(dfFrequencyGroupABC\\\n",
    "          .join(dfMonetaryGroupABC, [\"CustomerID\"]), [\"CustomerID\"])\n",
    "dfRecencyFrequencyMonetary.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:48 WARN TaskSetManager: Stage 143 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:50 WARN TaskSetManager: Stage 144 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:52 WARN TaskSetManager: Stage 145 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:01:55 WARN TaskSetManager: Stage 146 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:01:58 WARN TaskSetManager: Stage 156 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:01 WARN TaskSetManager: Stage 157 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:03 WARN TaskSetManager: Stage 158 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:05 WARN TaskSetManager: Stage 159 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "|CustomerID|Recency|RecencyGroup|Frequency|FrequencyGroup|         Monetary|MonetaryGroup|\n",
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "|   14096.0|   4695|           A|     5111|             A|65164.78999999966|            A|\n",
      "+----------+-------+------------+---------+--------------+-----------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# датафрейм с Id только тех клиентов, у которых значения групп ААА:\n",
    "dfRFM_AAA = dfRecencyFrequencyMonetary.filter(\\\n",
    "    (dfRecencyFrequencyMonetary.RecencyGroup == \"A\") &\\\n",
    "    (dfRecencyFrequencyMonetary.FrequencyGroup == \"A\") &\\\n",
    "    (dfRecencyFrequencyMonetary.MonetaryGroup == \"A\")\n",
    ").orderBy(\"CustomerID\")\n",
    "dfRFM_AAA.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Добавьте итоговый столбец с “суммой” значений групп по каждому показателю \n",
    "    и сохраните в отдельный csv-файл Id только тех клиентов, у которых значения \n",
    "    групп ААА."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 20:02:09 WARN TaskSetManager: Stage 166 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:12 WARN TaskSetManager: Stage 167 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:14 WARN TaskSetManager: Stage 168 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/07 20:02:16 WARN TaskSetManager: Stage 169 contains a task of very large size (33883 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfRFM_AAA.write.options(header=True).mode(\"overwrite\").csv(\"./csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
